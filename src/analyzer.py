import re
from datetime import datetime
from collections import defaultdict
import json
import os
import hashlib
from openai import OpenAI
from dotenv import load_dotenv
try:
    from .config import Config
except ImportError:
    import sys
    import os
    # src ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    from config import Config

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class StockNewsAnalyzer:
    def __init__(self, cache_dir=None):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key or api_key == 'your_openai_api_key_here':
            print("Warning: OpenAI API key not configured. Please set OPENAI_API_KEY in .env file")
            self.client = None
        else:
            self.client = OpenAI(api_key=api_key)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.cache_dir = cache_dir or os.environ.get("CACHE_DIR", "cache")
        self.analysis_cache_dir = os.path.join(self.cache_dir, "analysis")
        if not os.path.exists(self.analysis_cache_dir):
            os.makedirs(self.analysis_cache_dir)
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ í‰ê°€ìš© í‚¤ì›Œë“œëŠ” ìœ ì§€
        self.rumor_indicators = [
            'ì¹´ë”ë¼', 'ì†Œë¬¸', 'ì¶”ì •', 'ì˜ˆìƒ', 'ê´€ì¸¡', 'ì „ë§', 'ê¸°ëŒ€', 'ì¶”ì¸¡', 'ë£¨ë¨¸',
            '~ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤', '~ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤', '~í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ', '~ê²ƒìœ¼ë¡œ ê´€ì¸¡',
            'ë¯¸í™•ì¸', 'ë¶ˆí™•ì‹¤', 'ê°€ëŠ¥ì„±', '~ë¼ëŠ” ì†Œì‹', 'ì—…ê³„ì— ë”°ë¥´ë©´'
        ]
        
        self.official_indicators = [
            'ê³µì‹œ', 'ë°œí‘œ', 'ë³´ê³ ì„œ', 'ì‹¤ì ë°œí‘œ', 'ì‚¬ì—…ë³´ê³ ì„œ', 'ë¶„ê¸°ë³´ê³ ì„œ',
            'ê³µì‹', 'í™•ì •', 'ê²°ì •', 'ì²´ê²°', 'ê³„ì•½ì„œ', 'í˜‘ì•½ì„œ', 'ì¦ê¶Œì‹ ê³ ì„œ',
            'ê°ì‚¬ë³´ê³ ì„œ', 'ì´ì‚¬íšŒê²°ì˜', 'ì£¼ì£¼ì´íšŒ'
        ]

    def analyze_sentiment_with_gpt(self, article):
        """GPT-4o minië¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        if not self.client:
            return self._fallback_sentiment_analysis(article)
        
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        
        prompt = f"""
ë‹¤ìŒ ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ìì ê´€ì ì—ì„œ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content}

ë¶„ì„ ê¸°ì¤€:
- positive: ì£¼ê°€ì— ê¸ì •ì  ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‚´ìš©
- negative: ì£¼ê°€ì— ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‚´ìš©  
- neutral: ì¤‘ë¦½ì ì´ê±°ë‚˜ ì˜í–¥ì´ ë¶ˆë¶„ëª…í•œ ë‚´ìš©

ì‘ë‹µ í˜•ì‹:
sentiment: [positive/negative/neutral]
confidence: [0.1-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„]
reasoning: [ë¶„ì„ ê·¼ê±°ë¥¼ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ]

ì˜ˆì‹œ:
sentiment: positive
confidence: 0.8
reasoning: ëŒ€ê·œëª¨ ìˆ˜ì£¼ ê³„ì•½ ì²´ê²°ë¡œ ë§¤ì¶œ ì¦ê°€ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì£¼ì‹ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì„ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )
            
            result_text = response.choices[0].message.content.strip()
            return self._parse_gpt_response(result_text)
            
        except Exception as e:
            print(f"GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._fallback_sentiment_analysis(article)

    def _parse_gpt_response(self, response_text):
        """GPT ì‘ë‹µ íŒŒì‹±"""
        try:
            lines = response_text.split('\n')
            sentiment = 'neutral'
            confidence = 0.5
            reasoning = ''
            
            for line in lines:
                line = line.strip()
                if line.startswith('sentiment:'):
                    sentiment = line.split(':', 1)[1].strip()
                elif line.startswith('confidence:'):
                    confidence = float(line.split(':', 1)[1].strip())
                elif line.startswith('reasoning:'):
                    reasoning = line.split(':', 1)[1].strip()
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'reasoning': reasoning
            }
            
        except Exception as e:
            print(f"GPT ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'sentiment': 'neutral',
                'confidence': 0.5,
                'reasoning': 'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'
            }

    def _fallback_sentiment_analysis(self, article):
        """GPT API ì‚¬ìš© ë¶ˆê°€ ì‹œ í´ë°± ë¶„ì„"""
        print("Warning: GPT API ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ í‚¤ì›Œë“œ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        positive_keywords = [
            'ìˆ˜ì£¼', 'ê³„ì•½', 'í˜‘ì•½', 'í˜‘ë ¥', 'íˆ¬ì', 'ì¦ê°€', 'ìƒìŠ¹', 'í˜¸ì¬', 'ì„ ì •', 'ì±„íƒ',
            'í™•ëŒ€', 'ì„±ì¥', 'ê°œë°œ', 'íŠ¹í—ˆ', 'ì¸ì¦', 'ìŠ¹ì¸', 'ì¶œì‹œ', 'ë¡ ì¹­', 'ë§¤ì¶œì¦ê°€',
            'ì‹¤ì ê°œì„ ', 'í‘ìì „í™˜', 'í‘ì', 'ì´ìµì¦ê°€', 'ë°°ë‹¹', 'ì£¼ê°€ìƒìŠ¹', 'ëª©í‘œì£¼ê°€ìƒí–¥'
        ]
        
        negative_keywords = [
            'ì†ì‹¤', 'ì ì', 'í•˜ë½', 'ê°ì†Œ', 'ì•…ì¬', 'ë¦¬ì½œ', 'ì¤‘ë‹¨', 'ì—°ê¸°', 'ì·¨ì†Œ', 'í•´ì•½',
            'ì†Œì†¡', 'ìˆ˜ì‚¬', 'ì¡°ì‚¬', 'ì²˜ë²Œ', 'ì œì¬', 'ê·œì œ', 'ìœ„ë°˜', 'ì‚¬ê³ ', 'ê²°í•¨',
            'ì‹¤ì ì•…í™”', 'ë§¤ì¶œê°ì†Œ', 'ì£¼ê°€í•˜ë½', 'ëª©í‘œì£¼ê°€í•˜í–¥', 'êµ¬ì¡°ì¡°ì •', 'ê°ì›'
        ]
        
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        full_text = f"{title} {content}".lower()
        
        positive_count = sum(1 for keyword in positive_keywords if keyword in full_text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in full_text)
        
        if positive_count > negative_count:
            return {'sentiment': 'positive', 'confidence': 0.6, 'reasoning': 'ê¸ì •ì  í‚¤ì›Œë“œ ë°œê²¬'}
        elif negative_count > positive_count:
            return {'sentiment': 'negative', 'confidence': 0.6, 'reasoning': 'ë¶€ì •ì  í‚¤ì›Œë“œ ë°œê²¬'}
        else:
            return {'sentiment': 'neutral', 'confidence': 0.5, 'reasoning': 'ì¤‘ë¦½ì  ë‚´ìš©'}

    def assess_reliability(self, article):
        """ë‰´ìŠ¤ ì‹ ë¢°ë„ í‰ê°€"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        full_text = f"{title} {content}".lower()
        
        official_score = sum(1 for indicator in self.official_indicators if indicator in full_text)
        rumor_score = sum(1 for indicator in self.rumor_indicators if indicator in full_text)
        
        if official_score >= 2:
            return 'high'
        elif official_score >= 1 and rumor_score == 0:
            return 'medium'
        elif rumor_score >= 2:
            return 'low'
        else:
            return 'medium'

    def _get_article_hash(self, article):
        """ê¸°ì‚¬ì˜ ê³ ìœ  í•´ì‹œê°’ ìƒì„± (ì œëª© + ë‚ ì§œ ê¸°ë°˜)"""
        title = article.get('title', '').strip().lower()
        date = article.get('date', '').strip()
        content = f"{title}_{date}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _load_analysis_cache(self, cache_key):
        """ë¶„ì„ ê²°ê³¼ ìºì‹œ ë¡œë“œ"""
        cache_file = os.path.join(self.analysis_cache_dir, f"{cache_key}.json")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return None
        return None
    
    def _save_analysis_cache(self, cache_key, data):
        """ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥"""
        cache_file = os.path.join(self.analysis_cache_dir, f"{cache_key}.json")
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _deduplicate_articles(self, articles):
        """ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ì œëª©ê³¼ ë‚ ì§œ ê¸°ë°˜)"""
        seen_hashes = set()
        unique_articles = []
        
        for article in articles:
            article_hash = self._get_article_hash(article)
            if article_hash not in seen_hashes:
                seen_hashes.add(article_hash)
                unique_articles.append(article)
            else:
                print(f"ì¤‘ë³µ ê¸°ì‚¬ ì œê±°: {article.get('title', '')[:50]}...")
        
        print(f"ì¤‘ë³µ ì œê±°: {len(articles)}ê°œ â†’ {len(unique_articles)}ê°œ")
        return unique_articles

    def analyze_news_batch(self, articles):
        """ë‰´ìŠ¤ ë°°ì¹˜ ë¶„ì„ (ì¤‘ë³µ ì œê±° ë° ìºì‹± í¬í•¨)"""
        try:
            from .web_utils import safe_print
        except ImportError:
            from web_utils import safe_print
        
        safe_print(f"ğŸ§  AI ê°ì • ë¶„ì„ ì‹œì‘: {len(articles)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ì¤‘ë³µ ê¸°ì‚¬ ì œê±°
        safe_print(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° ì¤‘...")
        articles = self._deduplicate_articles(articles)
        safe_print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(articles)}ê°œ ë‰´ìŠ¤")
        
        results = []
        cache_hits = 0
        api_calls = 0
        
        safe_print(f"ğŸ¤– GPT-4o minië¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„ ì‹œì‘...")
        
        for i, article in enumerate(articles, 1):
            article_hash = self._get_article_hash(article)
            
            # ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ í™•ì¸
            cached_analysis = self._load_analysis_cache(article_hash)
            if cached_analysis:
                results.append(cached_analysis)
                cache_hits += 1
                safe_print(f"ğŸ’¾ ìºì‹œ ì¬ì‚¬ìš© ({i}/{len(articles)}): {article.get('title', '')[:50]}...")
                continue
            
            safe_print(f"ğŸ” OpenAI API í˜¸ì¶œ ì¤‘ ({i}/{len(articles)}): {article.get('title', '')[:50]}...")
            
            # GPTë¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„
            try:
                sentiment_result = self.analyze_sentiment_with_gpt(article)
                api_calls += 1
                safe_print(f"âœ… AI ë¶„ì„ ì™„ë£Œ: ê°ì •={sentiment_result['sentiment']}, ì‹ ë¢°ë„={sentiment_result['confidence']:.2f}")
            except Exception as e:
                safe_print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {str(e)[:50]}...")
                continue
                
            reliability = self.assess_reliability(article)
            
            # ì‚¬ê±´ ìš”ì•½ ì¶”ê°€
            event_summary = self._extract_event_summary(article, sentiment_result)
            
            analysis = {
                'title': article.get('title', ''),
                'date': article.get('date', ''),
                'link': article.get('link', ''),
                'summary': article.get('summary', ''),
                'sentiment': sentiment_result['sentiment'],
                'confidence': sentiment_result['confidence'],
                'reasoning': sentiment_result['reasoning'],
                'event_summary': event_summary,
                'reliability': reliability,
                'score': self._calculate_score(sentiment_result['sentiment'], sentiment_result['confidence'], reliability),
                'hash': article_hash
            }
            
            # ë¶„ì„ ê²°ê³¼ ìºì‹œì— ì €ì¥
            safe_print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ ì¤‘...")
            self._save_analysis_cache(article_hash, analysis)
            results.append(analysis)
        
        safe_print(f"\nğŸ‰ ê°ì • ë¶„ì„ ì™„ë£Œ!")
        safe_print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì´ {len(articles)}ê°œ")
        safe_print(f"ğŸ’¾ ìºì‹œ ì¬ì‚¬ìš©: {cache_hits}ê°œ")
        safe_print(f"ğŸ¤– ìƒˆë¡œ ë¶„ì„: {api_calls}ê°œ")
        safe_print(f"ğŸ”¢ ìµœì¢… ê²°ê³¼: {len(results)}ê°œ")
        
        if results:
            # ê°ì •ë³„ í†µê³„
            positive = len([r for r in results if r['sentiment'] == 'positive'])
            negative = len([r for r in results if r['sentiment'] == 'negative'])
            neutral = len([r for r in results if r['sentiment'] == 'neutral'])
            safe_print(f"ğŸ“ˆ ê°ì • ë¶„í¬: ê¸ì • {positive}ê°œ, ë¶€ì • {negative}ê°œ, ì¤‘ë¦½ {neutral}ê°œ")
        
        return sorted(results, key=lambda x: (abs(x['score']), x['confidence']), reverse=True)
    
    def _extract_event_summary(self, article, sentiment_result):
        """ê¸°ì‚¬ì—ì„œ ì£¼ìš” ì‚¬ê±´ ìš”ì•½ ì¶”ì¶œ"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        
        # ì£¼ìš” ì‚¬ê±´ í‚¤ì›Œë“œ íŒ¨í„´
        event_patterns = {
            'contract': ['ê³„ì•½', 'ìˆ˜ì£¼', 'í˜‘ì•½', 'ì²´ê²°'],
            'investment': ['íˆ¬ì', 'ìê¸ˆ', 'ì¡°ë‹¬', 'ìœ ì¹˜'],
            'performance': ['ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ'],
            'technology': ['ê¸°ìˆ ', 'íŠ¹í—ˆ', 'ê°œë°œ', 'í˜ì‹ '],
            'partnership': ['ì œíœ´', 'íŒŒíŠ¸ë„ˆì‹­', 'í˜‘ë ¥', 'í•©ì‘'],
            'regulation': ['ê·œì œ', 'ì •ì±…', 'ë²•ë¥ ', 'ìŠ¹ì¸'],
            'market': ['ì‹œì¥', 'ì§„ì¶œ', 'í™•ëŒ€', 'ì ìœ ìœ¨'],
            'lawsuit': ['ì†Œì†¡', 'ë¶„ìŸ', 'ë²•ì •', 'ì¬íŒ'],
            'accident': ['ì‚¬ê³ ', 'ë¬¸ì œ', 'ê²°í•¨', 'ë¦¬ì½œ']
        }
        
        detected_events = []
        full_text = f"{title} {content}".lower()
        
        for event_type, keywords in event_patterns.items():
            if any(keyword in full_text for keyword in keywords):
                detected_events.append(event_type)
        
        # ì‚¬ê±´ ìš”ì•½ ìƒì„±
        if detected_events:
            event_desc = ', '.join(detected_events)
            return f"{sentiment_result['sentiment']} - {event_desc}"
        else:
            return f"{sentiment_result['sentiment']} - ì¼ë°˜ ë‰´ìŠ¤"

    def _calculate_score(self, sentiment, confidence, reliability):
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        base_scores = {
            'positive': 1,
            'negative': -1,
            'neutral': 0
        }
        
        reliability_multipliers = {
            'high': 2.0,
            'medium': 1.0,
            'low': 0.3
        }
        
        return base_scores[sentiment] * confidence * reliability_multipliers[reliability]

    def filter_significant_news(self, analyzed_news, min_score=0.3):
        """ì¤‘ìš”í•œ ë‰´ìŠ¤ í•„í„°ë§"""
        return [news for news in analyzed_news if abs(news['score']) >= min_score]

    def validate_factors_with_gpt4(self, stock_name, positive_news, negative_news, start_date, end_date):
        """GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜¸ì¬/ì•…ì¬ ìœ íš¨ì„± ê²€ì¦"""
        if not self.client:
            return {"validation": "GPT API ì‚¬ìš© ë¶ˆê°€", "positive_valid": positive_news, "negative_valid": negative_news}
        
        # ìºì‹œ í‚¤ ìƒì„±
        validation_key = f"validation_{stock_name}_{start_date}_{end_date}_{len(positive_news)}_{len(negative_news)}"
        validation_hash = hashlib.md5(validation_key.encode('utf-8')).hexdigest()
        
        # ìºì‹œì—ì„œ ê²€ì¦ ê²°ê³¼ í™•ì¸
        cached_validation = self._load_analysis_cache(f"validation_{validation_hash}")
        if cached_validation:
            print("ğŸ“‹ GPT-4o ê²€ì¦ ê²°ê³¼ ìºì‹œ ì¬ì‚¬ìš©")
            return cached_validation
        
        # í˜¸ì¬/ì•…ì¬ ìš”ì•½
        positive_summary = []
        for news in positive_news[:10]:  # ìƒìœ„ 10ê°œë§Œ
            positive_summary.append(f"- {news['title']} ({news['date']}) - {news['event_summary']}")
        
        negative_summary = []
        for news in negative_news[:10]:  # ìƒìœ„ 10ê°œë§Œ
            negative_summary.append(f"- {news['title']} ({news['date']}) - {news['event_summary']}")
        
        prompt = f"""
{stock_name} ì£¼ì‹ì— ëŒ€í•œ {start_date}ë¶€í„° {end_date}ê¹Œì§€ì˜ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ê²€ì¦í•´ì£¼ì„¸ìš”.

í˜¸ì¬ ìš”ì¸ë“¤:
{chr(10).join(positive_summary) if positive_summary else "ì—†ìŒ"}

ì•…ì¬ ìš”ì¸ë“¤:
{chr(10).join(negative_summary) if negative_summary else "ì—†ìŒ"}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê° í˜¸ì¬/ì•…ì¬ê°€ ì‹¤ì œë¡œ ì£¼ê°€ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ì •ë³´ì¸ì§€ í‰ê°€í•´ì£¼ì„¸ìš”:

1. êµ¬ì²´ì„±: êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê³„ì•½ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
2. ì‹œê¸°: ì‹¤ì œ ì‚¬ì—…ì— ì˜í–¥ì„ ì£¼ëŠ” ì‹œì ì´ ëª…í™•í•œê°€?
3. ì¤‘ìš”ì„±: íšŒì‚¬ì˜ ë§¤ì¶œì´ë‚˜ ìˆ˜ìµì„±ì— ì‹¤ì§ˆì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ê°€?
4. ì‹ ë¢°ì„±: ê³µì‹ ë°œí‘œë‚˜ í™•ì‹¤í•œ ì •ë³´ì›ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ê°€?

ì‘ë‹µ í˜•ì‹:
valid_positive: [ìœ íš¨í•œ í˜¸ì¬ ê°œìˆ˜]
valid_negative: [ìœ íš¨í•œ ì•…ì¬ ê°œìˆ˜]
positive_analysis: [í˜¸ì¬ë“¤ì˜ ìœ íš¨ì„± ë¶„ì„]
negative_analysis: [ì•…ì¬ë“¤ì˜ ìœ íš¨ì„± ë¶„ì„]
overall_assessment: [ì „ì²´ì ì¸ í‰ê°€]
"""

        try:
            print("GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ í˜¸ì¬/ì•…ì¬ ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì£¼ì‹ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ì˜ ì‹¤ì§ˆì ì¸ ì£¼ê°€ ì˜í–¥ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1000
            )
            
            result_text = response.choices[0].message.content.strip()
            validation_result = self._parse_validation_response(result_text)
            validation_result['raw_response'] = result_text
            
            # ê²€ì¦ ê²°ê³¼ ìºì‹œì— ì €ì¥
            self._save_analysis_cache(f"validation_{validation_hash}", validation_result)
            
            return validation_result
            
        except Exception as e:
            print(f"GPT-4o ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"validation": "ê²€ì¦ ì‹¤íŒ¨", "positive_valid": positive_news, "negative_valid": negative_news}
    
    def _parse_validation_response(self, response_text):
        """GPT-4o ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        try:
            print(f"íŒŒì‹± ì‹œì‘ - ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
            print(f"ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            
            lines = response_text.split('\n')
            result = {
                'valid_positive': 0,
                'valid_negative': 0,
                'positive_analysis': '',
                'negative_analysis': '',
                'overall_assessment': ''
            }
            
            # ë” ìœ ì—°í•œ íŒŒì‹± ë¡œì§
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # ì½œë¡ ì´ í¬í•¨ëœ ë¼ì¸ë§Œ ì²˜ë¦¬
                if ':' in line:
                    key_part = line.split(':', 1)[0].strip().lower()
                    value_part = line.split(':', 1)[1].strip()
                    
                    if 'valid_positive' in key_part or 'positive' in key_part and 'valid' in key_part:
                        numbers = ''.join(filter(str.isdigit, value_part))
                        result['valid_positive'] = int(numbers) if numbers else 0
                        print(f"í˜¸ì¬ íŒŒì‹±: {result['valid_positive']}")
                    elif 'valid_negative' in key_part or 'negative' in key_part and 'valid' in key_part:
                        numbers = ''.join(filter(str.isdigit, value_part))
                        result['valid_negative'] = int(numbers) if numbers else 0
                        print(f"ì•…ì¬ íŒŒì‹±: {result['valid_negative']}")
                    elif 'positive_analysis' in key_part or ('positive' in key_part and 'analysis' in key_part):
                        result['positive_analysis'] = value_part
                        print(f"í˜¸ì¬ ë¶„ì„ íŒŒì‹±: {value_part[:50]}...")
                    elif 'negative_analysis' in key_part or ('negative' in key_part and 'analysis' in key_part):
                        result['negative_analysis'] = value_part
                        print(f"ì•…ì¬ ë¶„ì„ íŒŒì‹±: {value_part[:50]}...")
                    elif 'overall_assessment' in key_part or 'overall' in key_part or 'assessment' in key_part or 'ì¢…í•©' in key_part:
                        result['overall_assessment'] = value_part
                        print(f"ì¢…í•© í‰ê°€ íŒŒì‹±: {value_part[:50]}...")
            
            # ë¹ˆ ê°’ì´ ìˆìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not result['positive_analysis'] or not result['negative_analysis'] or not result['overall_assessment']:
                print("ê¸°ë³¸ íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„...")
                result = self._extract_from_full_text(response_text, result)
            
            print(f"ìµœì¢… íŒŒì‹± ê²°ê³¼: {result}")
            return result
            
        except Exception as e:
            print(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"ì˜¤ë¥˜ ë°œìƒ ì‘ë‹µ: {response_text}")
            return {
                'valid_positive': 0,
                'valid_negative': 0,
                'positive_analysis': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'negative_analysis': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'overall_assessment': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}'
            }
    
    def _extract_from_full_text(self, text, current_result):
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë¶„ì„ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ë³„ë¡œ ë¶„ë¦¬
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para in paragraphs:
                if len(para) > 20:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ë½ë§Œ
                    if not current_result['positive_analysis'] and ('í˜¸ì¬' in para or 'positive' in para.lower()):
                        current_result['positive_analysis'] = para[:200] + "..." if len(para) > 200 else para
                    elif not current_result['negative_analysis'] and ('ì•…ì¬' in para or 'negative' in para.lower()):
                        current_result['negative_analysis'] = para[:200] + "..." if len(para) > 200 else para
                    elif not current_result['overall_assessment'] and ('ì¢…í•©' in para or 'overall' in para.lower() or 'ì „ì²´' in para):
                        current_result['overall_assessment'] = para[:200] + "..." if len(para) > 200 else para
            
            return current_result
        except:
            return current_result

    def generate_future_outlook(self, stock_name, analyzed_news, indicators_data, end_date):
        """ë¯¸ë˜ ì „ë§ ë¶„ì„ ìƒì„±"""
        if not self.client:
            return "GPT API ì‚¬ìš© ë¶ˆê°€ë¡œ ë¯¸ë˜ ì „ë§ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìºì‹œ í‚¤ ìƒì„±
        outlook_key = f"outlook_{stock_name}_{end_date}_{len(analyzed_news)}"
        outlook_hash = hashlib.md5(outlook_key.encode('utf-8')).hexdigest()
        
        # ìºì‹œì—ì„œ ì „ë§ ê²°ê³¼ í™•ì¸
        cached_outlook = self._load_analysis_cache(f"outlook_{outlook_hash}")
        if cached_outlook:
            print("ğŸ“‹ ë¯¸ë˜ ì „ë§ ìºì‹œ ì¬ì‚¬ìš©")
            return cached_outlook
        
        # ì£¼ìš” ì´ìŠˆ ìš”ì•½
        significant_news = [n for n in analyzed_news if abs(n['score']) >= 0.5][:5]
        news_summary = []
        for news in significant_news:
            news_summary.append(f"- {news['title']} ({news['sentiment']}, ì ìˆ˜: {news['score']:.1f})")
        
        # ê²½ì œì§€í‘œ ìš”ì•½
        indicators_summary = []
        for indicator, data in self._analyze_indicators(indicators_data).items():
            indicators_summary.append(f"- {indicator}: {data['current_value']} ({data['trend']} {data['change_percent']}%)")
        
        prompt = f"""
{stock_name}ì— ëŒ€í•œ {end_date} ì‹œì  ê¸°ì¤€ ë¯¸ë˜ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

í˜„ì¬ ìƒí™© ìš”ì•½:
ì£¼ìš” ë‰´ìŠ¤:
{chr(10).join(news_summary) if news_summary else "íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"}

ê²½ì œ ì§€í‘œ:
{chr(10).join(indicators_summary) if indicators_summary else "ê²½ì œì§€í‘œ ì •ë³´ ì—†ìŒ"}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. í˜„ì¬ ì‹œì  í‰ê°€: í˜„ì¬ ìƒí™©ì˜ ê°•ì ê³¼ ì•½ì 
2. ë‹¨ê¸° ì „ë§ (1-3ê°œì›”): ê°€ê¹Œìš´ ë¯¸ë˜ì— ì˜ˆìƒë˜ëŠ” ë³€í™”
3. ì¤‘ê¸° ì „ë§ (3-12ê°œì›”): ì¤‘ì¥ê¸°ì ìœ¼ë¡œ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì¸ë“¤
4. ì£¼ì˜ì‚¬í•­: íˆ¬ììê°€ ì£¼ì˜ê¹Šê²Œ ë´ì•¼ í•  ë¦¬ìŠ¤í¬ ìš”ì¸ë“¤

ì‘ë‹µ í˜•ì‹:
current_assessment: [í˜„ì¬ ì‹œì  í‰ê°€]
short_term_outlook: [ë‹¨ê¸° ì „ë§]
medium_term_outlook: [ì¤‘ê¸° ì „ë§]
risk_factors: [ì£¼ì˜ì‚¬í•­]
investment_recommendation: [íˆ¬ì ê´€ì  ì œì•ˆ]
"""

        try:
            print("GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ì „ë§ ë¶„ì„ ì¤‘...")
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì£¼ì‹ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ê´€ì ì—ì„œ ë¯¸ë˜ ì „ë§ì„ ì œì‹œí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1200
            )
            
            result_text = response.choices[0].message.content.strip()
            outlook_result = self._parse_outlook_response(result_text)
            outlook_result['raw_response'] = result_text
            
            # ì „ë§ ê²°ê³¼ ìºì‹œì— ì €ì¥
            self._save_analysis_cache(f"outlook_{outlook_hash}", outlook_result)
            
            return outlook_result
            
        except Exception as e:
            print(f"ë¯¸ë˜ ì „ë§ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ë¯¸ë˜ ì „ë§ ë¶„ì„ ì‹¤íŒ¨"
    
    def _parse_outlook_response(self, response_text):
        """ë¯¸ë˜ ì „ë§ ì‘ë‹µ íŒŒì‹±"""
        try:
            print(f"ë¯¸ë˜ ì „ë§ íŒŒì‹± ì‹œì‘ - ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
            print(f"ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            
            lines = response_text.split('\n')
            result = {
                'current_assessment': '',
                'short_term_outlook': '',
                'medium_term_outlook': '',
                'risk_factors': '',
                'investment_recommendation': ''
            }
            
            # ë” ìœ ì—°í•œ íŒŒì‹± ë¡œì§
            for line in lines:
                line = line.strip()
                if not line or ':' not in line:
                    continue
                    
                key_part = line.split(':', 1)[0].strip().lower()
                value_part = line.split(':', 1)[1].strip()
                
                if 'current_assessment' in key_part or ('current' in key_part and 'assessment' in key_part) or 'í˜„ì¬' in key_part:
                    result['current_assessment'] = value_part
                    print(f"í˜„ì¬ í‰ê°€ íŒŒì‹±: {value_part[:50]}...")
                elif 'short_term_outlook' in key_part or ('short' in key_part and 'term' in key_part) or 'ë‹¨ê¸°' in key_part:
                    result['short_term_outlook'] = value_part
                    print(f"ë‹¨ê¸° ì „ë§ íŒŒì‹±: {value_part[:50]}...")
                elif 'medium_term_outlook' in key_part or ('medium' in key_part and 'term' in key_part) or 'ì¤‘ê¸°' in key_part:
                    result['medium_term_outlook'] = value_part
                    print(f"ì¤‘ê¸° ì „ë§ íŒŒì‹±: {value_part[:50]}...")
                elif 'risk_factors' in key_part or 'risk' in key_part or 'ë¦¬ìŠ¤í¬' in key_part or 'ìœ„í—˜' in key_part:
                    result['risk_factors'] = value_part
                    print(f"ë¦¬ìŠ¤í¬ ìš”ì¸ íŒŒì‹±: {value_part[:50]}...")
                elif 'investment_recommendation' in key_part or ('investment' in key_part and 'recommendation' in key_part) or 'íˆ¬ì' in key_part:
                    result['investment_recommendation'] = value_part
                    print(f"íˆ¬ì ê´€ì  íŒŒì‹±: {value_part[:50]}...")
            
            # ë¹ˆ ê°’ì´ ìˆìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not all(result.values()):
                print("ê¸°ë³¸ íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„...")
                result = self._extract_outlook_from_full_text(response_text, result)
            
            print(f"ìµœì¢… ì „ë§ íŒŒì‹± ê²°ê³¼: {list(result.keys())}")
            return result
            
        except Exception as e:
            print(f"ì „ë§ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"ì˜¤ë¥˜ ë°œìƒ ì‘ë‹µ: {response_text}")
            return {
                'current_assessment': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'short_term_outlook': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'medium_term_outlook': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'risk_factors': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}',
                'investment_recommendation': f'íŒŒì‹± ì˜¤ë¥˜: {str(e)}'
            }
    
    def _extract_outlook_from_full_text(self, text, current_result):
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì „ë§ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ë³„ë¡œ ë¶„ë¦¬
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para in paragraphs:
                if len(para) > 20:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ë½ë§Œ
                    para_lower = para.lower()
                    if not current_result['current_assessment'] and ('í˜„ì¬' in para or 'current' in para_lower):
                        current_result['current_assessment'] = para[:300] + "..." if len(para) > 300 else para
                    elif not current_result['short_term_outlook'] and ('ë‹¨ê¸°' in para or 'short' in para_lower):
                        current_result['short_term_outlook'] = para[:300] + "..." if len(para) > 300 else para
                    elif not current_result['medium_term_outlook'] and ('ì¤‘ê¸°' in para or 'medium' in para_lower):
                        current_result['medium_term_outlook'] = para[:300] + "..." if len(para) > 300 else para
                    elif not current_result['risk_factors'] and ('ë¦¬ìŠ¤í¬' in para or 'risk' in para_lower or 'ìœ„í—˜' in para):
                        current_result['risk_factors'] = para[:300] + "..." if len(para) > 300 else para
                    elif not current_result['investment_recommendation'] and ('íˆ¬ì' in para or 'investment' in para_lower):
                        current_result['investment_recommendation'] = para[:300] + "..." if len(para) > 300 else para
            
            return current_result
        except:
            return current_result

    def generate_summary(self, analyzed_news, indicators_data):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        positive_news = [n for n in analyzed_news if n['sentiment'] == 'positive' and n['score'] >= 0.3]
        negative_news = [n for n in analyzed_news if n['sentiment'] == 'negative' and n['score'] <= -0.3]
        
        summary = {
            'analysis_date': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'total_articles': len(analyzed_news),
            'positive_factors': len(positive_news),
            'negative_factors': len(negative_news),
            'top_positive': positive_news[:10] if positive_news else [],
            'top_negative': sorted(negative_news, key=lambda x: x['score'])[:10] if negative_news else [],
            'economic_indicators': self._analyze_indicators(indicators_data),
            'overall_sentiment': self._calculate_overall_sentiment(analyzed_news),
            'conclusion': self._generate_conclusion(positive_news, negative_news, indicators_data)
        }
        
        return summary

    def _analyze_indicators(self, indicators_data):
        """ê²½ì œ ì§€í‘œ ë¶„ì„"""
        analysis = {}
        
        for indicator_name, data_list in indicators_data.items():
            if not data_list:
                continue
                
            latest_value = data_list[-1]['value'] if data_list else 0
            previous_value = data_list[-2]['value'] if len(data_list) > 1 else latest_value
            
            change = ((latest_value - previous_value) / previous_value * 100) if previous_value != 0 else 0
            
            analysis[indicator_name] = {
                'current_value': latest_value,
                'change_percent': round(change, 2),
                'trend': 'positive' if change > 0 else 'negative' if change < 0 else 'stable'
            }
        
        return analysis

    def _calculate_overall_sentiment(self, analyzed_news):
        """ì „ì²´ ê°ì • ê³„ì‚°"""
        if not analyzed_news:
            return 'neutral'
            
        total_score = sum(news['score'] for news in analyzed_news)
        avg_score = total_score / len(analyzed_news)
        
        if avg_score > 0.3:
            return 'positive'
        elif avg_score < -0.3:
            return 'negative'
        else:
            return 'neutral'

    def _generate_conclusion(self, positive_news, negative_news, indicators_data):
        """ê²°ë¡  ìƒì„±"""
        conclusion_parts = []
        
        if len(positive_news) > len(negative_news):
            conclusion_parts.append("ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì¸ ìš”ì¸ì´ ìš°ì„¸í•©ë‹ˆë‹¤.")
        elif len(negative_news) > len(positive_news):
            conclusion_parts.append("ì „ë°˜ì ìœ¼ë¡œ ë¶€ì •ì ì¸ ìš”ì¸ì´ ìš°ì„¸í•©ë‹ˆë‹¤.")
        else:
            conclusion_parts.append("ê¸ì •ì  ìš”ì¸ê³¼ ë¶€ì •ì  ìš”ì¸ì´ ê· í˜•ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.")
        
        rate_trend = self._get_rate_trend(indicators_data)
        if rate_trend:
            conclusion_parts.append(rate_trend)
        
        exchange_trend = self._get_exchange_trend(indicators_data)
        if exchange_trend:
            conclusion_parts.append(exchange_trend)
            
        return " ".join(conclusion_parts)

    def _get_rate_trend(self, indicators_data):
        """ê¸ˆë¦¬ ë™í–¥ ë¶„ì„"""
        if 'base_rate' not in indicators_data or not indicators_data['base_rate']:
            return None
            
        rate_data = indicators_data['base_rate']
        if len(rate_data) < 2:
            return None
            
        current = rate_data[-1]['value']
        previous = rate_data[-2]['value']
        
        if current > previous:
            return "ê¸°ì¤€ê¸ˆë¦¬ ìƒìŠ¹ìœ¼ë¡œ ê¸ˆìœµì£¼ì—ëŠ” ê¸ì •ì ì´ë‚˜ ëŒ€ì¶œ ì˜ì¡´ ê¸°ì—…ì—ëŠ” ë¶€ë‹´ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif current < previous:
            return "ê¸°ì¤€ê¸ˆë¦¬ í•˜ë½ìœ¼ë¡œ ìœ ë™ì„± ì¦ê°€ê°€ ì˜ˆìƒë˜ì–´ ì‹œì¥ì— ê¸ì •ì ì…ë‹ˆë‹¤."
        else:
            return None

    def _get_exchange_trend(self, indicators_data):
        """í™˜ìœ¨ ë™í–¥ ë¶„ì„"""
        if 'exchange_rate_usd' not in indicators_data or not indicators_data['exchange_rate_usd']:
            return None
            
        exchange_data = indicators_data['exchange_rate_usd']
        if len(exchange_data) < 2:
            return None
            
        current = exchange_data[-1]['value']
        previous = exchange_data[-2]['value']
        change_rate = ((current - previous) / previous) * 100
        
        if abs(change_rate) > 2:
            if change_rate > 0:
                return "ì›í™” ì•½ì„¸ë¡œ ìˆ˜ì¶œê¸°ì—…ì—ëŠ” ìœ ë¦¬í•˜ë‚˜ ìˆ˜ì… ì›ìì¬ ì˜ì¡´ ê¸°ì—…ì—ëŠ” ë¶€ë‹´ì´ ë©ë‹ˆë‹¤."
            else:
                return "ì›í™” ê°•ì„¸ë¡œ ìˆ˜ì… ì˜ì¡´ ê¸°ì—…ì—ëŠ” ìœ ë¦¬í•˜ë‚˜ ìˆ˜ì¶œê¸°ì—…ì—ëŠ” ë¶ˆë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            return None

    def generate_report(self, stock_name, analyzed_news, indicators_data, start_date=None, end_date=None, policy_impact=None):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± (GPT-4 ê²€ì¦ ë° ë¯¸ë˜ ì „ë§ í¬í•¨)"""
        if end_date is None:
            end_date = datetime.now()
        elif isinstance(end_date, str):
            end_date = datetime.strptime(end_date, "%Y-%m-%d")
        
        if start_date is None:
            start_date = end_date
        elif isinstance(start_date, str):
            start_date = datetime.strptime(start_date, "%Y-%m-%d")
            
        summary = self.generate_summary(analyzed_news, indicators_data)
        
        # GPT-4ë¥¼ ì‚¬ìš©í•œ í˜¸ì¬/ì•…ì¬ ìœ íš¨ì„± ê²€ì¦
        validation = self.validate_factors_with_gpt4(
            stock_name, 
            summary['top_positive'], 
            summary['top_negative'],
            start_date.strftime("%Y-%m-%d"),
            end_date.strftime("%Y-%m-%d")
        )
        
        # ë¯¸ë˜ ì „ë§ ë¶„ì„
        future_outlook = self.generate_future_outlook(
            stock_name,
            analyzed_news,
            indicators_data,
            end_date.strftime("%Y-%m-%d")
        )
        
        report = f"""
êµ­ë‚´ì£¼ì‹ ë¶„ì„ ë¦¬í¬íŠ¸: {stock_name}
{'=' * 70}

ğŸ“Š ë¶„ì„ ê°œìš”
ë¶„ì„ ê¸°ê°„: {start_date.strftime('%Yë…„ %mì›” %dì¼')} ~ {end_date.strftime('%Yë…„ %mì›” %dì¼')}
ë¶„ì„ ì‹œì : {summary['analysis_date']}
ì´ ë¶„ì„ ê¸°ì‚¬: {summary['total_articles']}ê°œ
ë¶„ì„ ë°©ë²•: GPT-4o mini ê°œë³„ ë¶„ì„ + GPT-4o ì¢…í•© ê²€ì¦

ğŸ“ˆ ê°œë³„ ê¸°ì‚¬ ë¶„ì„ ê²°ê³¼
ì£¼ìš” í˜¸ì¬ ìš”ì¸ ({summary['positive_factors']}ê°œ):
"""
        
        for i, news in enumerate(summary['top_positive'], 1):
            confidence = news.get('confidence', 0) * 100
            reasoning = news.get('reasoning', '')
            event_summary = news.get('event_summary', 'ì¼ë°˜ ë‰´ìŠ¤')
            report += f"{i}. {news['title']}\n"
            report += f"   ğŸ“… {news['date']} | ğŸ¯ {event_summary}\n"
            report += f"   ğŸ“Š ì‹ ë¢°ë„: {news['reliability']}, ì ìˆ˜: {news['score']:.1f}, AI í™•ì‹ ë„: {confidence:.0f}%\n"
            report += f"   ğŸ’­ ë¶„ì„: {reasoning}\n\n"
        
        report += f"ì£¼ìš” ì•…ì¬ ìš”ì¸ ({summary['negative_factors']}ê°œ):\n"
        
        for i, news in enumerate(summary['top_negative'], 1):
            confidence = news.get('confidence', 0) * 100
            reasoning = news.get('reasoning', '')
            event_summary = news.get('event_summary', 'ì¼ë°˜ ë‰´ìŠ¤')
            report += f"{i}. {news['title']}\n"
            report += f"   ğŸ“… {news['date']} | ğŸ¯ {event_summary}\n"
            report += f"   ğŸ“Š ì‹ ë¢°ë„: {news['reliability']}, ì ìˆ˜: {news['score']:.1f}, AI í™•ì‹ ë„: {confidence:.0f}%\n"
            report += f"   ğŸ’­ ë¶„ì„: {reasoning}\n\n"
        
        # GPT-4 ê²€ì¦ ê²°ê³¼ ì¶”ê°€
        if isinstance(validation, dict) and 'valid_positive' in validation:
            report += f"""
ğŸ” GPT-4o ì „ë¬¸ê°€ ê²€ì¦ ê²°ê³¼
ìœ íš¨í•œ í˜¸ì¬: {validation.get('valid_positive', 0)}ê°œ (ì´ {summary['positive_factors']}ê°œ ì¤‘)
ìœ íš¨í•œ ì•…ì¬: {validation.get('valid_negative', 0)}ê°œ (ì´ {summary['negative_factors']}ê°œ ì¤‘)

ğŸ“‹ í˜¸ì¬ ê²€ì¦ ì˜ê²¬: {validation.get('positive_analysis', 'ë¶„ì„ ì—†ìŒ')}
ğŸ“‹ ì•…ì¬ ê²€ì¦ ì˜ê²¬: {validation.get('negative_analysis', 'ë¶„ì„ ì—†ìŒ')}
ğŸ“‹ ì¢…í•© í‰ê°€: {validation.get('overall_assessment', 'í‰ê°€ ì—†ìŒ')}
"""

        report += "\nğŸ“Š ê±°ì‹œê²½ì œ ì§€í‘œ í˜„í™©:\n"
        
        for indicator, data in summary['economic_indicators'].items():
            indicator_names = {
                'base_rate': 'ê¸°ì¤€ê¸ˆë¦¬',
                'cpi': 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 
                'exchange_rate_usd': 'ë‹¬ëŸ¬í™˜ìœ¨',
                'exchange_rate_jpy': 'ì—”í™˜ìœ¨',
                'exchange_rate_eur': 'ìœ ë¡œí™˜ìœ¨'
            }
            
            name = indicator_names.get(indicator, indicator)
            trend_symbol = "ğŸ“ˆ" if data['trend'] == 'positive' else "ğŸ“‰" if data['trend'] == 'negative' else "ğŸ“Š"
            
            report += f"  {trend_symbol} {name}: {data['current_value']} ({data['change_percent']:+.1f}%)\n"
        
        # ì •ì±… ë° ì •ì¹˜ì  ë¶„ì„ ì¶”ê°€
        if policy_impact and isinstance(policy_impact, dict):
            report += f"""
ğŸ›ï¸ ì •ë¶€ ì •ì±… ë° ì •ì¹˜ì  ì˜í–¥ ë¶„ì„
"""
            
            # ì •ì±… ê´€ë ¨ ë‰´ìŠ¤
            if policy_impact.get('policy_news'):
                report += f"ğŸ“‹ ì •ì±… ê´€ë ¨ ë‰´ìŠ¤: {len(policy_impact['policy_news'])}ê°œ\n"
                for i, news in enumerate(policy_impact['policy_news'][:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    report += f"  {i}. {news.get('title', '')[:60]}...\n"
                    report += f"     ğŸ“… {news.get('date', '')}\n"
            
            # ì •ì¹˜ì  ë‰´ìŠ¤
            if policy_impact.get('political_news'):
                report += f"\nğŸ—³ï¸ ì •ì¹˜ì  ë‰´ìŠ¤: {len(policy_impact['political_news'])}ê°œ\n"
                for i, news in enumerate(policy_impact['political_news'][:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    report += f"  {i}. {news.get('title', '')[:60]}...\n"
                    report += f"     ğŸ“… {news.get('date', '')}\n"
            
            # ì •ì±… ì˜í–¥ í‰ê°€
            if policy_impact.get('market_impact'):
                impact = policy_impact['market_impact']
                impact_symbol = "ğŸŸ¢" if impact == 'positive' else "ğŸ”´" if impact == 'negative' else "ğŸŸ¡"
                report += f"\nğŸ“Š ì •ì±… ì‹œì¥ ì˜í–¥ë„: {impact_symbol} {impact.upper()}\n"
            
            if policy_impact.get('sector_relevance'):
                relevance = policy_impact['sector_relevance']
                relevance_symbol = "ğŸ¯" if relevance == 'high' else "ğŸ“" if relevance == 'medium' else "ğŸ“Œ"
                report += f"ğŸ¯ ì—…ì¢… ì—°ê´€ì„±: {relevance_symbol} {relevance.upper()}\n"
            
            if policy_impact.get('key_factors'):
                report += f"ğŸ”‘ í•µì‹¬ ìš”ì¸: {', '.join(policy_impact['key_factors'])}\n"
        
        # ë¯¸ë˜ ì „ë§ ì¶”ê°€
        if isinstance(future_outlook, dict):
            report += f"""
ğŸ”® ë¯¸ë˜ ì „ë§ ë¶„ì„ ({end_date.strftime('%Y-%m-%d')} ê¸°ì¤€)

ğŸ“ í˜„ì¬ ì‹œì  í‰ê°€: {future_outlook.get('current_assessment', 'í‰ê°€ ì—†ìŒ')}

â³ ë‹¨ê¸° ì „ë§ (1-3ê°œì›”): {future_outlook.get('short_term_outlook', 'ì „ë§ ì—†ìŒ')}

ğŸ“… ì¤‘ê¸° ì „ë§ (3-12ê°œì›”): {future_outlook.get('medium_term_outlook', 'ì „ë§ ì—†ìŒ')}

âš ï¸  ì£¼ìš” ë¦¬ìŠ¤í¬: {future_outlook.get('risk_factors', 'ìœ„í—˜ìš”ì¸ ì—†ìŒ')}

ğŸ’¡ íˆ¬ì ê´€ì : {future_outlook.get('investment_recommendation', 'ì œì•ˆ ì—†ìŒ')}
"""

        # Timeline ë¶„ì„ ì¶”ê°€
        print("ğŸ“… Timeline ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘...")

        report += f"""
ğŸ“ ì¢…í•© ì˜ê²¬
ì „ì²´ ì‹œì¥ ì‹¬ë¦¬: {summary['overall_sentiment'].upper()}
{summary['conclusion']}


â€» ë³¸ ë¶„ì„ì€ OpenAI ë¥¼ í™œìš©í•œ AI ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.
â€» íˆ¬ì ê²°ì •ì— ëŒ€í•œ ëª¨ë“  ì±…ì„ì€ ê°œì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
"""
        
        report += "\n" + "=" * 70
        
        return report

    def generate_timeline_analysis(self, analyzed_news, end_date=None):
        """ì‹œê°„ìˆœ í˜¸ì¬/ì•…ì¬ ìœ íš¨ì„± ë¶„ì„ ìƒì„±"""
        if end_date is None:
            end_date = datetime.now()
        elif isinstance(end_date, str):
            end_date = datetime.strptime(end_date, "%Y-%m-%d")
        
        print("ğŸ“… Timeline ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ë‚ ì§œë³„ë¡œ ë‰´ìŠ¤ ê·¸ë£¹í™”
        daily_issues = self._extract_daily_issues(analyzed_news)
        
        # í˜„ì¬ ì´ìŠˆ ìƒíƒœ ì—…ë°ì´íŠ¸
        current_issues = self._update_current_issues(daily_issues, end_date)
        
        # í˜„ì¬ ì‹œì ì—ì„œì˜ ìœ íš¨ì„± ë¶„ì„
        timeline_result = self._analyze_current_validity(current_issues, end_date)
        
        return timeline_result
    
    def _extract_daily_issues(self, analyzed_news):
        """ë‚ ì§œë³„ ì´ìŠˆ ì¶”ì¶œ ë° ë¶„ë¥˜"""
        daily_issues = defaultdict(list)
        
        issue_categories = {
            'ì—…ê³„ ë™í–¥': ['ì—…ê³„', 'ë™í–¥', 'íŠ¸ë Œë“œ', 'ì‹œì¥ì ìœ ìœ¨', 'ê²½ìŸ', 'ê¸°ìˆ í˜ì‹ '],
            'ì •ì±…/ê·œì œ': ['ì •ì±…', 'ê·œì œ', 'ë²•ë¥ ', 'ì •ë¶€', 'ìŠ¹ì¸', 'í—ˆê°€', 'ì œì¬'],
            'í•´ì™¸ í˜‘ë ¥': ['í•´ì™¸', 'ìˆ˜ì¶œ', 'í•´ì™¸ì§„ì¶œ', 'ê¸€ë¡œë²Œ', 'êµ­ì œ', 'í˜‘ë ¥'],
            'ì‹œì¥ ë³€í™”': ['ì‹œì¥', 'ìˆ˜ìš”', 'ê³µê¸‰', 'ê°€ê²©', 'ì†Œë¹„ì', 'ë§ˆì¼€íŒ…'],
            'ê¸ˆìœµ ì´ìŠˆ': ['íˆ¬ì', 'ìê¸ˆ', 'ëŒ€ì¶œ', 'ê¸ˆë¦¬', 'ë°°ë‹¹', 'ìœ ìƒì¦ì'],
            'ê¸°íƒ€': []
        }
        
        for news in analyzed_news:
            try:
                news_date = datetime.strptime(news['date'], "%Y-%m-%d")
                date_key = news_date.strftime("%Y-%m-%d")
                
                # ì´ìŠˆ ë¶„ë¥˜
                title_content = f"{news['title']} {news.get('summary', '')}".lower()
                issue_type = 'ê¸°íƒ€'
                
                for category, keywords in issue_categories.items():
                    if category != 'ê¸°íƒ€' and any(keyword in title_content for keyword in keywords):
                        issue_type = category
                        break
                
                issue_data = {
                    'title': news['title'],
                    'sentiment': news['sentiment'],
                    'score': news['score'],
                    'confidence': news['confidence'],
                    'reasoning': news['reasoning'],
                    'reliability': news['reliability'],
                    'issue_type': issue_type,
                    'date': news_date
                }
                
                daily_issues[date_key].append(issue_data)
                
            except (ValueError, KeyError) as e:
                print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {news.get('date', 'N/A')} - {e}")
                continue
        
        return dict(daily_issues)
    
    def _update_current_issues(self, daily_issues, current_date):
        """í˜„ì¬ ì´ìŠˆ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        current_issues = {
            'ì—…ê³„ ë™í–¥': [],
            'ì •ì±…/ê·œì œ': [],
            'í•´ì™¸ í˜‘ë ¥': [],
            'ì‹œì¥ ë³€í™”': [],
            'ê¸ˆìœµ ì´ìŠˆ': [],
            'ê¸°íƒ€': []
        }
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì²˜ë¦¬
        sorted_dates = sorted(daily_issues.keys())
        
        for date_key in sorted_dates:
            issues = daily_issues[date_key]
            for issue in issues:
                category = issue['issue_type']
                current_issues[category].append(issue)
        
        return current_issues
    
    def _analyze_current_validity(self, current_issues, analysis_date):
        """í˜„ì¬ ì‹œì ì—ì„œ ì´ìŠˆë“¤ì˜ ìœ íš¨ì„± ë¶„ì„"""
        timeline_analysis = {
            'analysis_date': analysis_date.strftime("%Y-%m-%d"),
            'categories': {},
            'summary': {}
        }
        
        total_valid_issues = 0
        total_issues = 0
        
        for category, issues in current_issues.items():
            if not issues:
                continue
                
            category_analysis = {
                'total_issues': len(issues),
                'valid_issues': [],
                'outdated_issues': [],
                'summary': ''
            }
            
            for issue in issues:
                days_elapsed = (analysis_date - issue['date']).days
                validity_status = self._assess_issue_validity(issue, days_elapsed)
                
                issue_with_validity = issue.copy()
                issue_with_validity.update(validity_status)
                
                if validity_status['is_valid']:
                    category_analysis['valid_issues'].append(issue_with_validity)
                else:
                    category_analysis['outdated_issues'].append(issue_with_validity)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ ìƒì„±
            valid_count = len(category_analysis['valid_issues'])
            total_count = len(issues)
            
            category_analysis['summary'] = f"{total_count}ê°œ ì´ìŠˆ ì¤‘ {valid_count}ê°œê°€ í˜„ì¬ ìœ íš¨í•¨"
            
            timeline_analysis['categories'][category] = category_analysis
            total_valid_issues += valid_count
            total_issues += total_count
        
        # ì „ì²´ ìš”ì•½
        timeline_analysis['summary'] = {
            'total_issues': total_issues,
            'valid_issues': total_valid_issues,
            'validity_rate': round((total_valid_issues / total_issues * 100) if total_issues > 0 else 0, 1)
        }
        
        return timeline_analysis
    
    def _assess_issue_validity(self, issue, days_elapsed):
        """ê°œë³„ ì´ìŠˆì˜ ìœ íš¨ì„± í‰ê°€"""
        # ê¸°ë³¸ ìœ íš¨ì„± ê¸°ì¤€ (ì¼ìˆ˜ ê¸°ë°˜)
        if days_elapsed <= 7:
            validity_level = "ë§¤ìš° ìœ íš¨"
            is_valid = True
        elif days_elapsed <= 30:
            validity_level = "ìœ íš¨"
            is_valid = True
        elif days_elapsed <= 90:
            validity_level = "ì¼ë¶€ ìœ íš¨"
            is_valid = True
        else:
            validity_level = "ìœ íš¨ì„± ë‚®ìŒ"
            is_valid = False
        
        # ì´ìŠˆ íƒ€ì…ë³„ ì¡°ì •
        issue_type = issue['issue_type']
        if issue_type == 'ì •ì±…/ê·œì œ':
            # ì •ì±…ì€ ë” ì˜¤ë˜ ìœ íš¨
            if days_elapsed <= 180:
                is_valid = True
        elif issue_type == 'ì—…ê³„ ë™í–¥':
            # ì—…ê³„ ë™í–¥ì€ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ìœ íš¨ê¸°ê°„
            if days_elapsed > 60:
                is_valid = False
                validity_level = "ìœ íš¨ì„± ë‚®ìŒ"
        
        # ì‹ ë¢°ë„ ë° ì ìˆ˜ ê¸°ë°˜ ì¡°ì •
        reliability_factor = {
            'high': 1.2,
            'medium': 1.0,
            'low': 0.8
        }.get(issue['reliability'], 1.0)
        
        score_magnitude = abs(issue['score'])
        if score_magnitude >= 1.0 and reliability_factor >= 1.0:
            # ê³ ë“ì  + ê³ ì‹ ë¢°ë„ëŠ” ìœ íš¨ê¸°ê°„ ì—°ì¥
            if days_elapsed <= 45:
                is_valid = True
        
        return {
            'days_elapsed': days_elapsed,
            'validity_level': validity_level,
            'is_valid': is_valid,
            'reliability_factor': reliability_factor
        }
    
    def format_timeline_report(self, timeline_analysis):
        """Timeline ë¶„ì„ ê²°ê³¼ë¥¼ ë¦¬í¬íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        report = f"""
ğŸ“… Timeline í˜¸ì¬/ì•…ì¬ ìœ íš¨ì„± ë¶„ì„
{'=' * 50}

ë¶„ì„ ê¸°ì¤€ì¼: {timeline_analysis['analysis_date']}
ì „ì²´ ì´ìŠˆ: {timeline_analysis['summary']['total_issues']}ê°œ
í˜„ì¬ ìœ íš¨ ì´ìŠˆ: {timeline_analysis['summary']['valid_issues']}ê°œ
ìœ íš¨ì„± ë¹„ìœ¨: {timeline_analysis['summary']['validity_rate']}%

"""
        
        for category, data in timeline_analysis['categories'].items():
            if data['total_issues'] == 0:
                continue
                
            report += f"\nğŸ·ï¸  {category} ({data['summary']})\n"
            report += "â”€" * 40 + "\n"
            
            # ìœ íš¨í•œ ì´ìŠˆë“¤
            if data['valid_issues']:
                report += "âœ… í˜„ì¬ ìœ íš¨í•œ ì´ìŠˆë“¤:\n"
                for i, issue in enumerate(data['valid_issues'], 1):
                    sentiment_icon = "ğŸ“ˆ" if issue['sentiment'] == 'positive' else "ğŸ“‰" if issue['sentiment'] == 'negative' else "ğŸ“Š"
                    validity_icon = "ğŸŸ¢" if issue['validity_level'] == "ë§¤ìš° ìœ íš¨" else "ğŸŸ¡" if issue['validity_level'] == "ìœ íš¨" else "ğŸŸ "
                    
                    report += f"  {i}. {sentiment_icon} {issue['title'][:50]}...\n"
                    report += f"     {validity_icon} {issue['validity_level']} ({issue['days_elapsed']}ì¼ ê²½ê³¼)\n"
                    report += f"     ğŸ“Š ì ìˆ˜: {issue['score']:.1f}, ì‹ ë¢°ë„: {issue['reliability']}\n"
                    report += f"     ğŸ’­ {issue['reasoning'][:80]}...\n\n"
            
            # ìœ íš¨ì„±ì´ ë‚®ì€ ì´ìŠˆë“¤
            if data['outdated_issues']:
                report += "âŒ ìœ íš¨ì„±ì´ ë‚®ì€ ì´ìŠˆë“¤:\n"
                for i, issue in enumerate(data['outdated_issues'], 1):
                    sentiment_icon = "ğŸ“ˆ" if issue['sentiment'] == 'positive' else "ğŸ“‰" if issue['sentiment'] == 'negative' else "ğŸ“Š"
                    
                    report += f"  {i}. {sentiment_icon} {issue['title'][:50]}...\n"
                    report += f"     â° {issue['validity_level']} ({issue['days_elapsed']}ì¼ ê²½ê³¼)\n\n"
        
        report += f"""
ğŸ“Š Timeline ë¶„ì„ ìš”ì•½:
â€¢ ì´ {timeline_analysis['summary']['total_issues']}ê°œ ì´ìŠˆ ì¤‘ {timeline_analysis['summary']['valid_issues']}ê°œê°€ í˜„ì¬ë„ ìœ íš¨í•¨
â€¢ ìœ íš¨ì„± íŒë‹¨ ê¸°ì¤€: ì¼ë°˜ ì´ìŠˆ 30ì¼, ì •ì±…/ê·œì œ 180ì¼, ì—…ê³„ë™í–¥ 60ì¼
â€¢ ê³ ë“ì Â·ê³ ì‹ ë¢°ë„ ì´ìŠˆëŠ” ìœ íš¨ê¸°ê°„ ì—°ì¥ ì ìš©
â€¢ ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì´ìŠˆì˜ ì£¼ê°€ ì˜í–¥ë ¥ì€ ê°ì†Œí•¨

âš ï¸  íˆ¬ì ì°¸ê³ ì‚¬í•­:
- ìµœê·¼ 7ì¼ ì´ë‚´ ì´ìŠˆê°€ í˜„ì¬ ì£¼ê°€ì— ê°€ì¥ í° ì˜í–¥
- 30ì¼ ì´ìƒ ê²½ê³¼í•œ ì´ìŠˆëŠ” ì´ë¯¸ ì£¼ê°€ì— ë°˜ì˜ë˜ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ
- ì •ì±…/ê·œì œ ê´€ë ¨ ì´ìŠˆëŠ” ì¤‘ì¥ê¸°ì  ê´€ì ì—ì„œ ì§€ì† ëª¨ë‹ˆí„°ë§ í•„ìš”
"""
        
        return report