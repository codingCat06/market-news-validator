import requests
from datetime import datetime, timedelta
import json
import os
import time
import hashlib
from dotenv import load_dotenv

load_dotenv()

try:
    from .config import Config
except ImportError:
    import sys
    import os
    # src ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    from config import Config


class PolicyAnalyzer:
    def __init__(self, cache_dir=None):
        self.cache_dir = cache_dir or os.getenv("CACHE_DIR", "cache")
        self.policy_cache_dir = os.path.join(self.cache_dir, "policy")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„¤ì •
        self.naver_client_id = os.getenv("NAVER_CLIENT_ID")
        self.naver_client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.naver_search_url = "https://openapi.naver.com/v1/search/news.json"
        
        self.session = requests.Session()
        
        # ë„¤ì´ë²„ API í—¤ë” ì„¤ì •
        if self.naver_client_id and self.naver_client_secret:
            self.session.headers.update({
                'X-Naver-Client-Id': self.naver_client_id,
                'X-Naver-Client-Secret': self.naver_client_secret,
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
        
        if not os.path.exists(self.policy_cache_dir):
            os.makedirs(self.policy_cache_dir)
        
        # ë™ì  ì •ì±… ê²€ìƒ‰ì–´ ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
        self.policy_categories = {
            "monetary": ["ê¸ˆë¦¬", "í†µí™”ì •ì±…", "ê¸°ì¤€ê¸ˆë¦¬", "ì–‘ì ì™„í™”"],
            "fiscal": ["ì¬ì •ì •ì±…", "êµ­ê°€ì˜ˆì‚°", "ì¶”ê²½", "ì„¸ì œê°œí¸", "ë²•ì¸ì„¸", "ì†Œë“ì„¸"],
            "financial": ["ê¸ˆìœµì •ì±…", "ê¸ˆìœµê·œì œ", "ìë³¸ì‹œì¥", "ì¦ê¶Œê±°ë˜ì„¸", "ê¸ˆìœµíˆ¬ìì—…"],
            "real_estate": ["ë¶€ë™ì‚°ì •ì±…", "ì£¼íƒê³µê¸‰", "ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ë¶„ì–‘ê°€ìƒí•œì œ"],
            "industrial": ["ì‚°ì—…ì •ì±…", "ì‹ ì„±ì¥ë™ë ¥", "ê·¸ë¦°ë‰´ë”œ", "ë””ì§€í„¸ë‰´ë”œ", "K-ë‰´ë”œ"],
            "energy": ["ì—ë„ˆì§€ì •ì±…", "ì‹ ì¬ìƒì—ë„ˆì§€", "ì›ì „ì •ì±…", "íƒ„ì†Œì¤‘ë¦½", "ê·¸ë¦°í…"],
            "trade": ["í†µìƒì •ì±…", "FTA", "ìˆ˜ì¶œì§€ì›", "ê´€ì„¸ì •ì±…", "ë¬´ì—­í˜‘ì •"],
            "tech": ["ê³¼í•™ê¸°ìˆ ì •ì±…", "R&Díˆ¬ì", "ë°˜ë„ì²´", "AIì •ì±…", "ë°ì´í„°ì •ì±…"],
            "healthcare": ["ë³´ê±´ì˜ë£Œì •ì±…", "ì˜ë£Œê¸°ê¸°", "ë°”ì´ì˜¤", "ì œì•½ì •ì±…", "ì›ê²©ì˜ë£Œ"],
            "construction": ["ê±´ì„¤ì •ì±…", "SOCíˆ¬ì", "ë„ì‹œì¬ìƒ", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "ì¸í”„ë¼"],
            "automotive": ["ìë™ì°¨ì •ì±…", "ì „ê¸°ì°¨", "ìˆ˜ì†Œì°¨", "ììœ¨ì£¼í–‰", "ëª¨ë¹Œë¦¬í‹°"],
            "finance_sector": ["ì€í–‰ì •ì±…", "ë³´í—˜ì •ì±…", "ì¦ê¶Œì •ì±…", "í•€í…Œí¬", "ë””ì§€í„¸ê¸ˆìœµ"]
        }
        
        # ì •ì¹˜ì  ì´ìŠˆ ì¹´í…Œê³ ë¦¬
        self.political_categories = {
            "governance": ["êµ­ì •ê°ì‚¬", "ì •ë¶€ì •ì±…", "ëŒ€í†µë ¹", "êµ­ë¬´ì´ë¦¬"],
            "legislation": ["êµ­íšŒ", "ë²•ì•ˆ", "ê°œì •ì•ˆ", "êµ­ì •ê°ì‚¬"],
            "election": ["ì„ ê±°", "ì •ì¹˜", "ì—¬ë‹¹", "ì•¼ë‹¹", "ì •ì¹˜ì "],
            "international": ["ì™¸êµ", "í†µìƒ", "êµ­ì œê´€ê³„", "í•œë¯¸", "í•œì¤‘", "í•œì¼"]
        }

    def _get_cache_key(self, start_date, end_date, category):
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{category}_{start_date}_{end_date}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def _load_cache(self, cache_key):
        """ìºì‹œ ë°ì´í„° ë¡œë“œ"""
        cache_file = os.path.join(self.policy_cache_dir, f"{cache_key}.json")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ìºì‹œê°€ 24ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸
                    cache_time = datetime.fromisoformat(data.get('cache_time', '2020-01-01T00:00:00'))
                    if datetime.now() - cache_time < timedelta(hours=24):
                        return data
            except:
                pass
        return None

    def _save_cache(self, cache_key, data):
        """ìºì‹œ ë°ì´í„° ì €ì¥"""
        cache_file = os.path.join(self.policy_cache_dir, f"{cache_key}.json")
        data['cache_time'] = datetime.now().isoformat()
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ì •ì±… ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _parse_article_date(self, date_str):
        """ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        try:
            if "," in date_str and "+" in date_str:
                from email.utils import parsedate_to_datetime
                parsed_date = parsedate_to_datetime(date_str)
                return parsed_date.replace(tzinfo=None)
            else:
                for fmt in ["%Y%m%d", "%Y-%m-%d", "%Y.%m.%d"]:
                    try:
                        return datetime.strptime(date_str, fmt)
                    except:
                        continue
                return datetime.now()
        except Exception as e:
            return datetime.now()

    def _remove_html_tags(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        import re
        clean_text = re.sub('<.*?>', '', text)
        clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
        return clean_text.strip()

    def collect_policy_news(self, start_date, end_date, stock_name=None):
        """ì •ë¶€ ì •ì±… ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì£¼ì‹ë³„ ë§ì¶¤í˜•)"""
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, "%Y-%m-%d")
        if isinstance(end_date, str):
            end_date = datetime.strptime(end_date, "%Y-%m-%d")

        # ì£¼ì‹ëª…ì„ í¬í•¨í•œ ìºì‹œ í‚¤ ìƒì„±
        cache_suffix = f"_{stock_name}" if stock_name else ""
        cache_key = self._get_cache_key(start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"), f"policy{cache_suffix}")
        
        # ìºì‹œ í™•ì¸
        cached_data = self._load_cache(cache_key)
        if cached_data:
            print(f"ğŸ“‹ ì •ì±… ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì‚¬ìš© ({stock_name or 'ì¼ë°˜'})")
            return cached_data.get('articles', [])

        if not self.naver_client_id or not self.naver_client_secret:
            print("âš ï¸  ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì •ì±… ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
            return []

        # ë™ì  í‚¤ì›Œë“œ ìƒì„±
        if stock_name:
            keywords = self._generate_dynamic_keywords(stock_name)
            print(f"ğŸ›ï¸ {stock_name} ë§ì¶¤í˜• ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            print(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}...")
        else:
            # ê¸°ë³¸ ì •ì±… í‚¤ì›Œë“œ ì‚¬ìš©
            keywords = []
            for category in ["monetary", "fiscal", "financial"]:
                keywords.extend(self.policy_categories[category][:2])
            print("ğŸ›ï¸ ì¼ë°˜ ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        all_articles = []

        for keyword in keywords[:8]:  # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            try:
                search_params = {
                    'query': keyword,
                    'display': 20,  # í‚¤ì›Œë“œë‹¹ 20ê°œ
                    'start': 1,
                    'sort': 'date'
                }

                response = self.session.get(self.naver_search_url, params=search_params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    if 'items' in data:
                        for item in data['items']:
                            try:
                                title = self._remove_html_tags(item['title'])
                                description = self._remove_html_tags(item['description'])
                                pub_date = self._parse_article_date(item['pubDate'])
                                
                                if start_date <= pub_date <= end_date + timedelta(days=1):
                                    article = {
                                        'title': title,
                                        'summary': description[:200] + "..." if len(description) > 200 else description,
                                        'date': pub_date.strftime("%Y-%m-%d %H:%M"),
                                        'link': item['link'],
                                        'keyword': keyword,
                                        'category': 'policy'
                                    }
                                    all_articles.append(article)
                            except:
                                continue
                
                time.sleep(0.5)  # API ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
                continue

        # ì¤‘ë³µ ì œê±°
        unique_articles = self._remove_duplicates(all_articles)
        
        # ìºì‹œ ì €ì¥
        cache_data = {'articles': unique_articles}
        self._save_cache(cache_key, cache_data)
        
        print(f"ğŸ›ï¸ ì •ì±… ë‰´ìŠ¤ {len(unique_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles

    def collect_political_news(self, start_date, end_date):
        """ì •ì¹˜ì  ì´ìŠˆ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, "%Y-%m-%d")
        if isinstance(end_date, str):
            end_date = datetime.strptime(end_date, "%Y-%m-%d")

        cache_key = self._get_cache_key(start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"), "political")
        
        # ìºì‹œ í™•ì¸
        cached_data = self._load_cache(cache_key)
        if cached_data:
            print("ğŸ“‹ ì •ì¹˜ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
            return cached_data.get('articles', [])

        if not self.naver_client_id or not self.naver_client_secret:
            print("âš ï¸  ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì •ì¹˜ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
            return []

        # ë™ì  ì •ì¹˜ í‚¤ì›Œë“œ ìƒì„±
        keywords = self._generate_political_keywords()
        print("ğŸ—³ï¸ ì •ì¹˜ì  ì´ìŠˆ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        print(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:4])}...")
        
        all_articles = []

        for keyword in keywords[:6]:  # ìµœëŒ€ 6ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            try:
                search_params = {
                    'query': keyword,
                    'display': 15,  # í‚¤ì›Œë“œë‹¹ 15ê°œ
                    'start': 1,
                    'sort': 'date'
                }

                response = self.session.get(self.naver_search_url, params=search_params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    if 'items' in data:
                        for item in data['items']:
                            try:
                                title = self._remove_html_tags(item['title'])
                                description = self._remove_html_tags(item['description'])
                                pub_date = self._parse_article_date(item['pubDate'])
                                
                                if start_date <= pub_date <= end_date + timedelta(days=1):
                                    article = {
                                        'title': title,
                                        'summary': description[:200] + "..." if len(description) > 200 else description,
                                        'date': pub_date.strftime("%Y-%m-%d %H:%M"),
                                        'link': item['link'],
                                        'keyword': keyword,
                                        'category': 'political'
                                    }
                                    all_articles.append(article)
                            except:
                                continue
                
                time.sleep(0.5)  # API ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"ì •ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
                continue

        # ì¤‘ë³µ ì œê±°
        unique_articles = self._remove_duplicates(all_articles)
        
        # ìºì‹œ ì €ì¥
        cache_data = {'articles': unique_articles}
        self._save_cache(cache_key, cache_data)
        
        print(f"ğŸ›ï¸ ì •ì¹˜ ë‰´ìŠ¤ {len(unique_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles

    def _remove_duplicates(self, articles):
        """ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title_key = article['title'].strip().lower()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        return sorted(unique_articles, key=lambda x: x['date'], reverse=True)

    def analyze_policy_impact(self, stock_name, policy_news, political_news):
        """ì •ì±… ë° ì •ì¹˜ì  ì˜í–¥ ë¶„ì„"""
        impact_analysis = {
            'policy_summary': self._summarize_policy_news(policy_news),
            'political_summary': self._summarize_political_news(political_news),
            'market_impact': self._assess_market_impact(policy_news + political_news),
            'sector_relevance': self._assess_sector_relevance(stock_name, policy_news + political_news)
        }
        
        return impact_analysis

    def _summarize_policy_news(self, policy_news):
        """ì •ì±… ë‰´ìŠ¤ ìš”ì•½"""
        if not policy_news:
            return "ë¶„ì„ ê¸°ê°„ ë‚´ ì£¼ìš” ì •ì±… ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í‚¤ì›Œë“œë³„ ë¶„ë¥˜
        keyword_count = {}
        for news in policy_news:
            keyword = news.get('keyword', 'ê¸°íƒ€')
            keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        # ì£¼ìš” ì •ì±… ì´ìŠˆ ì¶”ì¶œ
        top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:3]
        
        summary = f"ì£¼ìš” ì •ì±… ì´ìŠˆ: "
        summary += ", ".join([f"{kw}({cnt}ê±´)" for kw, cnt in top_keywords])
        
        return summary

    def _summarize_political_news(self, political_news):
        """ì •ì¹˜ ë‰´ìŠ¤ ìš”ì•½"""
        if not political_news:
            return "ë¶„ì„ ê¸°ê°„ ë‚´ ì£¼ìš” ì •ì¹˜ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í‚¤ì›Œë“œë³„ ë¶„ë¥˜
        keyword_count = {}
        for news in political_news:
            keyword = news.get('keyword', 'ê¸°íƒ€')
            keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        # ì£¼ìš” ì •ì¹˜ ì´ìŠˆ ì¶”ì¶œ
        top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:3]
        
        summary = f"ì£¼ìš” ì •ì¹˜ ì´ìŠˆ: "
        summary += ", ".join([f"{kw}({cnt}ê±´)" for kw, cnt in top_keywords])
        
        return summary

    def _assess_market_impact(self, all_news):
        """ì‹œì¥ ì˜í–¥ë„ í‰ê°€"""
        if not all_news:
            return "ì¤‘ë¦½"
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜í–¥ë„ í‰ê°€
        positive_keywords = ['ì™„í™”', 'ì§€ì›', 'í˜œíƒ', 'í™•ëŒ€', 'íˆ¬ì', 'ì„±ì¥', 'ê°œì„ ']
        negative_keywords = ['ê·œì œ', 'ê°•í™”', 'ì œì¬', 'ì¶•ì†Œ', 'ì‚­ê°', 'ë¶€ë‹´', 'ìš°ë ¤']
        
        positive_score = 0
        negative_score = 0
        
        for news in all_news:
            title_content = (news.get('title', '') + ' ' + news.get('summary', '')).lower()
            
            for keyword in positive_keywords:
                if keyword in title_content:
                    positive_score += 1
            
            for keyword in negative_keywords:
                if keyword in title_content:
                    negative_score += 1
        
        if positive_score > negative_score:
            return "ê¸ì •ì "
        elif negative_score > positive_score:
            return "ë¶€ì •ì "
        else:
            return "ì¤‘ë¦½ì "

    def _assess_sector_relevance(self, stock_name, all_news):
        """ì„¹í„°ë³„ ì—°ê´€ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ ì„¹í„° í‚¤ì›Œë“œ ë§¤í•‘
        sector_keywords = {
            'ê¸ˆìœµ': ['ì€í–‰', 'ì¦ê¶Œ', 'ë³´í—˜', 'ì¹´ë“œ', 'ê¸ˆìœµ', 'ëŒ€ì¶œ', 'ì‹ ìš©'],
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'ê±´ì„¤', 'ì•„íŒŒíŠ¸', 'ì£¼íƒ', 'í† ì§€'],
            'ê¸°ìˆ ': ['IT', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì¸ê³µì§€ëŠ¥', 'AI', 'ë°˜ë„ì²´', 'ì „ì'],
            'ì œì¡°': ['ì œì¡°', 'ìƒì‚°', 'ê³µì¥', 'ìë™ì°¨', 'í™”í•™', 'ì² ê°•'],
            'ì—ë„ˆì§€': ['ì „ë ¥', 'ê°€ìŠ¤', 'ì„ìœ ', 'ì›ìë ¥', 'ì‹ ì¬ìƒ'],
            'ìœ í†µ': ['ìœ í†µ', 'ë°±í™”ì ', 'ë§ˆíŠ¸', 'ì‡¼í•‘', 'ì†Œë§¤']
        }
        
        relevance_scores = {}
        
        for sector, keywords in sector_keywords.items():
            score = 0
            for news in all_news:
                title_content = (news.get('title', '') + ' ' + news.get('summary', '')).lower()
                for keyword in keywords:
                    if keyword in title_content:
                        score += 1
            relevance_scores[sector] = score
        
        # ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ ì„¹í„°
        if relevance_scores:
            top_sector = max(relevance_scores.items(), key=lambda x: x[1])
            if top_sector[1] > 0:
                return f"{top_sector[0]} ì„¹í„°ì™€ ì—°ê´€ì„± ë†’ìŒ ({top_sector[1]}ê±´)"
        
        return "íŠ¹ì • ì„¹í„°ì™€ì˜ ì—°ê´€ì„± ë‚®ìŒ"
    
    def _generate_dynamic_keywords(self, stock_name):
        """ì£¼ì‹ë³„ ë§ì¶¤í˜• ì •ì±… ê²€ìƒ‰ì–´ ë™ì  ìƒì„±"""
        base_keywords = []
        
        # ì£¼ì‹ëª…ìœ¼ë¡œ ì—…ì¢… ì¶”ì • ë° ê´€ë ¨ ì •ì±… í‚¤ì›Œë“œ ì„ íƒ
        stock_lower = stock_name.lower()
        
        # ê¸ˆìœµì—…
        if any(keyword in stock_lower for keyword in ['ì€í–‰', 'ê¸ˆìœµ', 'ë³´í—˜', 'ì¦ê¶Œ', 'ì¹´ë“œ', 'ìºí”¼íƒˆ']):
            base_keywords.extend(self.policy_categories["financial"])
            base_keywords.extend(self.policy_categories["finance_sector"])
            
        # ê±´ì„¤ì—…
        elif any(keyword in stock_lower for keyword in ['ê±´ì„¤', 'ê±´ì¶•', 'í† ëª©', 'ì£¼íƒ', 'ì•„íŒŒíŠ¸']):
            base_keywords.extend(self.policy_categories["construction"])
            base_keywords.extend(self.policy_categories["real_estate"])
            
        # ì „ë ¥/ì—ë„ˆì§€
        elif any(keyword in stock_lower for keyword in ['ì „ë ¥', 'ì „ê¸°', 'ì—ë„ˆì§€', 'ë°œì „', 'ì›ì „', 'ê°€ìŠ¤']):
            base_keywords.extend(self.policy_categories["energy"])
            
        # ìë™ì°¨
        elif any(keyword in stock_lower for keyword in ['ìë™ì°¨', 'ëª¨í„°', 'íƒ€ì´ì–´', 'ë¶€í’ˆ']):
            base_keywords.extend(self.policy_categories["automotive"])
            
        # IT/ê¸°ìˆ 
        elif any(keyword in stock_lower for keyword in ['ì „ì', 'ë°˜ë„ì²´', 'ì†Œí”„íŠ¸ì›¨ì–´', 'it', 'í†µì‹ ', 'ì¸í„°ë„·']):
            base_keywords.extend(self.policy_categories["tech"])
            
        # ë°”ì´ì˜¤/ì œì•½
        elif any(keyword in stock_lower for keyword in ['ë°”ì´ì˜¤', 'ì œì•½', 'ì˜ë£Œ', 'ë³‘ì›']):
            base_keywords.extend(self.policy_categories["healthcare"])
            
        # ê¸°ë³¸ ì •ì±… í‚¤ì›Œë“œ ì¶”ê°€ (ëª¨ë“  ì£¼ì‹ì— ê³µí†µ ì ìš©)
        base_keywords.extend(self.policy_categories["monetary"][:2])  # ê¸ˆë¦¬, í†µí™”ì •ì±…
        base_keywords.extend(self.policy_categories["fiscal"][:2])    # ì¬ì •ì •ì±…, êµ­ê°€ì˜ˆì‚°
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 12ê°œ í‚¤ì›Œë“œë¡œ ì œí•œ
        unique_keywords = list(set(base_keywords))
        return unique_keywords[:12]
    
    def _generate_political_keywords(self):
        """ì •ì¹˜ì  ì´ìŠˆ ê²€ìƒ‰ì–´ ìƒì„±"""
        political_keywords = []
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œë§Œ ì„ íƒ
        for category, keywords in self.political_categories.items():
            political_keywords.extend(keywords[:2])  # ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ 2ê°œ
            
        return political_keywords[:8]  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
