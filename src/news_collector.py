import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import json
import os
from urllib.parse import urljoin, quote
import time
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

try:
    from .config import Config, UserAgent
except ImportError:
    from config import Config, UserAgent


class NewsCollector:
    def __init__(self, cache_dir=None):
        self.cache_dir = cache_dir or os.getenv("CACHE_DIR", "cache")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„¤ì •
        self.naver_client_id = os.getenv("NAVER_CLIENT_ID")
        self.naver_client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.naver_search_url = "https://openapi.naver.com/v1/search/news.json"
        
        self.session = requests.Session()
        
        # ë„¤ì´ë²„ API í—¤ë” ì„¤ì •
        if self.naver_client_id and self.naver_client_secret:
            self.session.headers.update({
                'X-Naver-Client-Id': self.naver_client_id,
                'X-Naver-Client-Secret': self.naver_client_secret,
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
        else:
            print("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

    def _get_cache_filename(self, stock_name, start_date, end_date):
        return f"{self.cache_dir}/news_{stock_name}_{start_date}_{end_date}.json"

    def _load_cached_data(self, stock_name, start_date, end_date):
        cache_file = self._get_cache_filename(stock_name, start_date, end_date)
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None

    def _save_to_cache(self, stock_name, start_date, end_date, data):
        cache_file = self._get_cache_filename(stock_name, start_date, end_date)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _parse_article_date(self, date_str):
        """ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        try:
            # ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹: "Mon, 01 Jan 2025 12:00:00 +0900"
            if "," in date_str and "+" in date_str:
                # RFC 2822 í˜•ì‹
                from email.utils import parsedate_to_datetime
                parsed_date = parsedate_to_datetime(date_str)
                # timezone-naiveë¡œ ë³€í™˜
                return parsed_date.replace(tzinfo=None)
            else:
                # ê¸°ë³¸ í˜•ì‹ë“¤
                for fmt in ["%Y%m%d", "%Y-%m-%d", "%Y.%m.%d"]:
                    try:
                        return datetime.strptime(date_str, fmt)
                    except:
                        continue
                return datetime.now()
        except Exception as e:
            print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return datetime.now()

    def _extract_article_summary(self, content):
        if len(content) <= 150:
            return content
        
        sentences = content.split('.')
        summary = ""
        for sentence in sentences:
            if len(summary + sentence) <= 150:
                summary += sentence + "."
            else:
                break
        return summary.strip()

    def collect_news(self, stock_name, start_date, end_date):
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, "%Y-%m-%d")
        if isinstance(end_date, str):
            end_date = datetime.strptime(end_date, "%Y-%m-%d")

        cached_data = self._load_cached_data(stock_name, start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"))
        if cached_data:
            print(f"ìºì‹œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {stock_name}")
            return cached_data

        if not self.naver_client_id or not self.naver_client_secret:
            print("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            print("ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”: https://developers.naver.com/")
            return []

        articles = []
        
        try:
            # ë‹¨ì¼ ê²€ìƒ‰ì–´ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì´ì „ ë°©ì‹ìœ¼ë¡œ ë³µì›)
            max_pages = int(os.getenv('NEWS_MAX_PAGES', 3))
            display_per_page = 100
            max_articles = 50  # ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ
            
            for page in range(max_pages):
                if len(articles) >= max_articles:
                    print(f"ï¿½ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜({max_articles}ê°œ)ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                    
                start_index = page * display_per_page + 1
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API íŒŒë¼ë¯¸í„° (ê¸°ë³¸ ê²€ìƒ‰ì–´ë¡œ ë³µì›)
                search_params = {
                    'query': f'{stock_name} ì£¼ì‹',
                    'display': display_per_page,
                    'start': start_index,
                    'sort': 'date'  # ë‚ ì§œìˆœ ì •ë ¬
                }
                
                print(f"ğŸ“„ í˜ì´ì§€ {page + 1} ê²€ìƒ‰ ì¤‘... (ê²€ìƒ‰ì–´: {stock_name} ì£¼ì‹)")
                response = self.session.get(self.naver_search_url, params=search_params, timeout=10)
                
                if response.status_code != 200:
                    print(f"âŒ API ì˜¤ë¥˜: {response.status_code}")
                    continue
                
                data = response.json()
                
                if 'items' not in data or not data['items']:
                    print(f"â­ï¸ í˜ì´ì§€ {page + 1}ì— ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                print(f"ğŸ“Š í˜ì´ì§€ {page + 1}ì—ì„œ {len(data['items'])}ê°œ ë°œê²¬")
                
                date_filtered_out = 0
                parsing_errors = 0
                page_articles = 0
                
                for item in data['items']:
                    if len(articles) >= max_articles:
                        break
                        
                    try:
                        # HTML íƒœê·¸ ì œê±°
                        title = self._remove_html_tags(item['title'])
                        description = self._remove_html_tags(item['description'])
                        
                        # ë‚ ì§œ íŒŒì‹±
                        pub_date = self._parse_article_date(item['pubDate'])
                        
                        # ì‚¬ìš©ìê°€ ì§€ì •í•œ ë‚ ì§œ ë²”ìœ„ ë‚´ì˜ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
                        if start_date <= pub_date <= end_date + timedelta(days=1):
                            article = {
                                'title': title,
                                'summary': description[:150] + "..." if len(description) > 150 else description,
                                'date': pub_date.strftime("%Y-%m-%d %H:%M"),
                                'link': item['link'],
                                'content': description
                            }
                            articles.append(article)
                            page_articles += 1
                            if page_articles <= 5:
                                print(f"âœ… ìˆ˜ì§‘: {title[:40]}...")
                        else:
                            date_filtered_out += 1
                    
                    except Exception as e:
                        parsing_errors += 1
                        continue
                
                print(f"ğŸ“ˆ í˜ì´ì§€ {page + 1} ê²°ê³¼: ìˆ˜ì§‘ {page_articles}ê°œ, ë‚ ì§œ í•„í„°ë§ {date_filtered_out}ê°œ, ì˜¤ë¥˜ {parsing_errors}ê°œ")
                
                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
                if page_articles == 0 and page > 1:
                    print(f"â¹ï¸ ì—°ì†í•´ì„œ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                    
                # API ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                if page < max_pages - 1:
                    time.sleep(float(os.getenv('NEWS_REQUEST_DELAY', 1.0)))
            
        except Exception as e:
            print(f"ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

        articles = self._remove_duplicates(articles)
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ ë° ìƒìœ„ 50ê°œ ì„ íƒ (ì´ë¯¸ ìˆ˜ì§‘ ì‹œ ì œí•œí–ˆì§€ë§Œ ì¶”ê°€ ë³´ì¥)
        articles = self._prioritize_and_limit_articles(articles, max_articles=50)
        
        self._save_to_cache(stock_name, start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"), articles)
        
        print(f"\nğŸ‰ {stock_name} ë‰´ìŠ¤ ì´ {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ìµœëŒ€ 50ê°œ ì œí•œ)")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        return articles

    def _remove_html_tags(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        import re
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub('<.*?>', '', text)
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
        return clean_text.strip()

    def _remove_duplicates(self, articles):
        """ë” ì •êµí•œ ì¤‘ë³µ ì œê±° (ì™„ì „íˆ ê°™ì€ ì œëª©ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ì œëª© ëª¨ë‘ ì²˜ë¦¬)"""
        import re
        
        unique_articles = []
        seen_titles = set()
        
        for article in articles:
            title = article['title'].strip()
            title_normalized = re.sub(r'[^\w\s]', '', title.lower())  # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì†Œë¬¸ì ë³€í™˜
            title_words = set(title_normalized.split())
            
            # ê¸°ì¡´ ì œëª©ë“¤ê³¼ ìœ ì‚¬ë„ ì²´í¬
            is_duplicate = False
            for seen_title in seen_titles:
                seen_words = set(seen_title.split())
                
                # ë‘ ì œëª©ì´ 80% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                if len(title_words) > 0 and len(seen_words) > 0:
                    intersection = title_words.intersection(seen_words)
                    similarity = len(intersection) / max(len(title_words), len(seen_words))
                    
                    if similarity > 0.8:  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                seen_titles.add(title_normalized)
                unique_articles.append(article)
            else:
                print(f"ğŸ”„ ìœ ì‚¬ ì¤‘ë³µ ì œê±°: {title[:40]}...")
        
        print(f"ğŸ“ ì¤‘ë³µ ì œê±° ê²°ê³¼: {len(articles)}ê°œ â†’ {len(unique_articles)}ê°œ")
        return sorted(unique_articles, key=lambda x: x['date'], reverse=True)

    def _prioritize_and_limit_articles(self, articles, max_articles=50):
        """ë‰´ìŠ¤ë¥¼ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ì„ íƒ"""
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        for article in articles:
            score = self._calculate_importance_score(article)
            article['importance_score'] = score
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
        sorted_articles = sorted(articles, key=lambda x: x['importance_score'], reverse=True)
        
        # ìƒìœ„ max_articlesê°œë§Œ ì„ íƒ
        limited_articles = sorted_articles[:max_articles]
        
        # importance_score í•„ë“œ ì œê±° (ì €ì¥ ì‹œ ë¶ˆí•„ìš”)
        for article in limited_articles:
            article.pop('importance_score', None)
        
        if len(articles) > max_articles:
            print(f"ğŸ” ì¤‘ìš”ë„ ìˆœ ì •ë ¬: {len(articles)}ê°œ â†’ ìƒìœ„ {max_articles}ê°œ ì„ íƒ")
        
        return limited_articles
    
    def _calculate_importance_score(self, article):
        """ë‰´ìŠ¤ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        title = article.get('title', '').lower()
        content = article.get('content', '').lower()
        
        score = 0
        
        # 1. ì¤‘ìš” í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        important_keywords = {
            'ì‹¤ì ': 10, 'ë§¤ì¶œ': 8, 'ì˜ì—…ì´ìµ': 8, 'ìˆœì´ìµ': 8,
            'ìˆ˜ì£¼': 9, 'ê³„ì•½': 8, 'í˜‘ì•½': 7, 'íˆ¬ì': 8,
            'í•©ë³‘': 10, 'ì¸ìˆ˜': 9, 'M&A': 10,
            'ì‹ ì œí’ˆ': 7, 'ì¶œì‹œ': 6, 'ê°œë°œ': 6,
            'ê³µì‹œ': 9, 'ë°œí‘œ': 6, 'ë³´ê³ ì„œ': 5,
            'ì¦ì': 8, 'ë°°ë‹¹': 7, 'ë¶„í• ': 8,
            'ì†Œì†¡': 7, 'ë¶„ìŸ': 6, 'ê·œì œ': 6,
            'íŠ¹í—ˆ': 6, 'ì¸ì¦': 5, 'ìŠ¹ì¸': 6
        }
        
        for keyword, weight in important_keywords.items():
            if keyword in title:
                score += weight * 2  # ì œëª©ì— ìˆìœ¼ë©´ 2ë°° ê°€ì¤‘ì¹˜
            elif keyword in content:
                score += weight
        
        # 2. ë‚ ì§œ ì ìˆ˜ (ìµœì‹  ë‰´ìŠ¤ ìš°ëŒ€)
        try:
            article_date = datetime.strptime(article['date'], "%Y-%m-%d %H:%M")
            days_ago = (datetime.now() - article_date).days
            date_score = max(0, 10 - days_ago)  # ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            score += date_score
        except:
            pass
        
        # 3. ì œëª© ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ì ì ˆí•œ ê¸¸ì´)
        title_length = len(article.get('title', ''))
        if 20 <= title_length <= 60:
            score += 3
        elif 10 <= title_length <= 80:
            score += 1
        
        # 4. ê²€ìƒ‰ì–´ ë§¤ì¹˜ë„ (íŠ¹ì • ê²€ìƒ‰ì–´ë¡œ ì°¾ì€ ê²½ìš° ê°€ì¤‘ì¹˜)
        search_query = article.get('search_query', '')
        if 'ì£¼ì‹' in search_query:
            score += 2
        elif 'íˆ¬ì' in search_query:
            score += 3
        
        return score

    def get_recent_news(self, stock_name, days=7):
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        return self.collect_news(stock_name, start_date, end_date)